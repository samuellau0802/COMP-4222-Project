{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86c48cb",
   "metadata": {
    "id": "d86c48cb"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef4235b",
   "metadata": {
    "id": "eef4235b"
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import itertools\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "from startup_data_set import COMP4222Dataset_hetero\n",
    "from PredictorClasses import *\n",
    "from CustomMetrics import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97ba3dc4",
   "metadata": {
    "id": "97ba3dc4",
    "outputId": "737d2c0b-16a9-44c1-a660-d8ffd2202984"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5147a43",
   "metadata": {
    "id": "b5147a43"
   },
   "source": [
    "### Hypermeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e5bde2",
   "metadata": {
    "id": "13e5bde2"
   },
   "outputs": [],
   "source": [
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 32\n",
    "output_dim = 16\n",
    "\n",
    "early_stopping = 10\n",
    "ep = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6446e8",
   "metadata": {
    "id": "1e6446e8"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8ceb2eb",
   "metadata": {
    "id": "f8ceb2eb",
    "outputId": "ececbf2a-f57c-4dc2-f94c-0a5e527cdb94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=25446, num_edges=45621,\n",
       "      ndata_schemes={'feat': Scheme(shape=(221,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
       "      edata_schemes={'feat': Scheme(shape=(23,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = COMP4222Dataset_hetero()[0]\n",
    "graph = dgl.to_homogeneous(graph,ndata=['feat'],edata=['feat'])\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e82188",
   "metadata": {
    "id": "b7e82188",
    "outputId": "7a598391-99cd-413c-9b6a-2c7f5c112ce3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_feats = graph.ndata['feat'].shape[1]\n",
    "in_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e76e323",
   "metadata": {
    "id": "7e76e323"
   },
   "source": [
    "## Generate Postitve Graph and Negative Graph with Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b21ea8e",
   "metadata": {
    "id": "3b21ea8e",
    "outputId": "b400e7f3-69b2-4202-8fe3-61448f1a7e73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=25446, num_edges=61943,\n",
       "      ndata_schemes={'feat': Scheme(shape=(221,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)}\n",
       "      edata_schemes={'feat': Scheme(shape=(23,), dtype=torch.float64), '_ID': Scheme(shape=(), dtype=torch.int64), '_TYPE': Scheme(shape=(), dtype=torch.int64)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CustomUtilities import generate_neg_graph, generate_pos_graph\n",
    "train_g, train_pos_g, val_pos_g, test_pos_g = \\\n",
    "    generate_pos_graph(graph, val_ratio, test_ratio)\n",
    "\n",
    "train_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfe9889",
   "metadata": {
    "id": "8dfe9889",
    "outputId": "dfbdad04-abca-410a-9efb-2274e89f661c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=25446, num_edges=36497,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_neg_g, val_neg_g, test_neg_g = \\\n",
    "    generate_neg_graph(graph, val_ratio, test_ratio)\n",
    "\n",
    "train_neg_g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38657b8",
   "metadata": {
    "id": "e38657b8"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a4f954",
   "metadata": {
    "id": "c3a4f954"
   },
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171f3f4d",
   "metadata": {
    "id": "171f3f4d"
   },
   "outputs": [],
   "source": [
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "\n",
    "class VGAEModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden1_dim, hidden2_dim):\n",
    "        super(VGAEModel, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden1_dim = hidden1_dim\n",
    "        self.hidden2_dim = hidden2_dim\n",
    "\n",
    "        layers = [\n",
    "            GraphConv(\n",
    "                self.in_dim,\n",
    "                self.hidden1_dim,\n",
    "                activation=F.relu,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "            GraphConv(\n",
    "                self.hidden1_dim,\n",
    "                self.hidden2_dim,\n",
    "                activation=lambda x: x,\n",
    "                allow_zero_in_degree=True,\n",
    "            ),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def encoder(self, g, features):\n",
    "        h = self.layers[0](g, features)\n",
    "        self.mean = self.layers[1](g, h)\n",
    "        self.log_std = self.layers[2](g, h)\n",
    "        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(\n",
    "            device\n",
    "        )\n",
    "        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(\n",
    "            device\n",
    "        )\n",
    "        return sampled_z\n",
    "\n",
    "    def decoder(self, z):\n",
    "        adj_rec = nn.ReLU()(torch.matmul(z, z.t()))\n",
    "        return adj_rec\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        z = self.encoder(g, features)\n",
    "        adj_rec = self.decoder(z)\n",
    "        return adj_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce225f9",
   "metadata": {
    "id": "5ce225f9"
   },
   "outputs": [],
   "source": [
    "vgae_model = VGAEModel(\n",
    "    in_feats,\n",
    "    n_hidden,\n",
    "    output_dim\n",
    ")\n",
    "\n",
    "pred = DotPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaf5ef5f",
   "metadata": {
    "id": "aaf5ef5f"
   },
   "outputs": [],
   "source": [
    "vgae_model = vgae_model.to(device)\n",
    "pred = pred.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ceeab3",
   "metadata": {
    "id": "88ceeab3"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c945ab1f",
   "metadata": {
    "id": "c945ab1f",
    "outputId": "9c5cda5f-00ff-4baf-be18-8a3b66dd1146"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.41 GiB (GPU 0; 8.00 GiB total capacity; 2.57 GiB already allocated; 1.06 GiB free; 4.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(itertools\u001b[38;5;241m.\u001b[39mchain(vgae_model\u001b[38;5;241m.\u001b[39mparameters(), pred\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mvgae_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     pos_score \u001b[38;5;241m=\u001b[39m pred(train_pos_g\u001b[38;5;241m.\u001b[39mto(device), h\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     13\u001b[0m     neg_score \u001b[38;5;241m=\u001b[39m pred(train_neg_g\u001b[38;5;241m.\u001b[39mto(device), h\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[1;32mD:\\Dennis\\ust\\year3_fall_sem\\COMP4222\\gnn_ws\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [8], line 51\u001b[0m, in \u001b[0;36mVGAEModel.forward\u001b[1;34m(self, g, features)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, features):\n\u001b[0;32m     50\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(g, features)\n\u001b[1;32m---> 51\u001b[0m     adj_rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adj_rec\n",
      "Cell \u001b[1;32mIn [8], line 46\u001b[0m, in \u001b[0;36mVGAEModel.decoder\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m---> 46\u001b[0m     adj_rec \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adj_rec\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.41 GiB (GPU 0; 8.00 GiB total capacity; 2.57 GiB already allocated; 1.06 GiB free; 4.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "train_AUC, val_AUC = [], []\n",
    "stop = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(vgae_model.parameters(), pred.parameters()), lr=0.05)\n",
    "\n",
    "for e in range(500):\n",
    "    # forward\n",
    "    h = vgae_model(train_g.to(device), train_g.ndata['feat'].float().to(device))\n",
    "    pos_score = pred(train_pos_g.to(device), h.to(device))\n",
    "    neg_score = pred(train_neg_g.to(device), h.to(device))\n",
    "    loss = compute_loss(pos_score, neg_score).to(device)\n",
    "    train_loss.append(loss.item())\n",
    "    train_AUC.append(compute_auc(pos_score, neg_score))\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    v_pos_score = pred(val_pos_g.to(device), h.to(device))\n",
    "    v_neg_score = pred(val_neg_g.to(device), h.to(device))\n",
    "    v_loss = compute_loss(v_pos_score, v_neg_score)\n",
    "    val_loss.append(v_loss.item())\n",
    "    val_AUC.append(compute_auc(v_pos_score, v_neg_score))\n",
    "\n",
    "    #verbose\n",
    "    if e % 1 == 0:\n",
    "        print('Epoch: {} \\t Train loss: {} \\t Val loss: {} \\t Train AUC: {} \\t Val AUC: {}'.format(e, round(loss.item(), 3), round(v_loss.item(), 3), round(train_AUC[-1],3), round(val_AUC[-1], 3)))\n",
    "\n",
    "\n",
    "    # early stopping\n",
    "    if e > 10:\n",
    "        if v_loss.item() > sum(val_loss[-5:])/5:\n",
    "            stop += 1\n",
    "        else: \n",
    "            stop = 0\n",
    "        if stop >= early_stopping:\n",
    "            print(\"Early Stopped at Epoch {}\".format(e))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25daee",
   "metadata": {
    "id": "fa25daee",
    "outputId": "f77fc419-263b-4ee6-abc7-cff09776a0c7"
   },
   "outputs": [],
   "source": [
    "# testing AUC\n",
    "with torch.no_grad():\n",
    "    pos_score = pred(test_pos_g, h)\n",
    "    neg_score = pred(test_neg_g, h)\n",
    "    print('AUC:', compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdf7c9",
   "metadata": {
    "id": "9fbdf7c9"
   },
   "source": [
    "### Result Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7e9cd",
   "metadata": {
    "id": "dbe7e9cd",
    "outputId": "b2465fd8-05ef-4f67-92c5-041796a01808"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss[50:])\n",
    "plt.plot(val_loss[50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db07e4",
   "metadata": {
    "id": "85db07e4",
    "outputId": "4d6ff091-19b0-445d-a454-cb341d7f3105"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_AUC)\n",
    "plt.plot(val_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f53aba",
   "metadata": {
    "id": "c6f53aba"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda238182c1de36151cdce09aa997c6697b1a1e480bed704816dfcebedb6bbfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
