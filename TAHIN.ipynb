{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import conv as dgl_conv\n",
    "from startup_data_set import *\n",
    "from PredictorClasses import *\n",
    "from CustomMetrics import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = COMP4222Dataset_hetero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedefd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edges(etype=\"i_s\")\n",
    "# Graph(num_nodes={'investor': 7594, 'startup': 17852},\n",
    "#       num_edges={('investor', 'i_s', 'startup'): 45621},\n",
    "#       metagraph=[('investor', 'startup', 'i_s')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575d7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.to_homogeneous(graph,ndata=['feat'],edata=['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = graph.edges()\n",
    "# give id for all edges then permutation\n",
    "eids = np.arange(graph.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "\n",
    "# use 10% as test set\n",
    "test_size = int(len(eids) * 0.1)\n",
    "train_size = graph.number_of_edges() - test_size\n",
    "\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "# Find all negative edges and split them for training and testing\n",
    "\n",
    "#use sparse matrix to save memory\n",
    "# ,shape = (torch.max(v)+1,torch.max(v)+1)\n",
    "adj = graph.adj(scipy_fmt='coo')\n",
    "adj_neg = 1 - adj.todense() - np.eye(graph.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0) # negative edge, we don't have edge\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), graph.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219609c0",
   "metadata": {},
   "source": [
    "# TAHIN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75533fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic attention in the metapath-based aggregation (the same as that in the HAN)\n",
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(SemanticAttention, self).__init__()\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Shape of z: (N, M , D*K)\n",
    "        N: number of nodes\n",
    "        M: number of metapath patterns\n",
    "        D: hidden_size\n",
    "        K: number of heads\n",
    "        \"\"\"\n",
    "        w = self.project(z).mean(0)  # (M, 1)\n",
    "        beta = torch.softmax(w, dim=0)  # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape)  # (N, M, 1)\n",
    "\n",
    "        return (beta * z).sum(1)  # (N, D * K)\n",
    "\n",
    "\n",
    "# Metapath-based aggregation (the same as the HANLayer)\n",
    "class HANLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, meta_path_patterns, in_size, out_size, layer_num_heads, dropout\n",
    "    ):\n",
    "        super(HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(len(meta_path_patterns)):\n",
    "            self.gat_layers.append(\n",
    "                GATConv(\n",
    "                    in_size,\n",
    "                    out_size,\n",
    "                    layer_num_heads,\n",
    "                    dropout,\n",
    "                    dropout,\n",
    "                    activation=F.elu,\n",
    "                    allow_zero_in_degree=True,\n",
    "                )\n",
    "            )\n",
    "        self.semantic_attention = SemanticAttention(\n",
    "            in_size=out_size * layer_num_heads\n",
    "        )\n",
    "        self.meta_path_patterns = list(\n",
    "            tuple(meta_path_pattern) for meta_path_pattern in meta_path_patterns\n",
    "        )\n",
    "\n",
    "        self._cached_graph = None\n",
    "        self._cached_coalesced_graph = {}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        semantic_embeddings = []\n",
    "        # obtain metapath reachable graph\n",
    "        if self._cached_graph is None or self._cached_graph is not g:\n",
    "            self._cached_graph = g\n",
    "            self._cached_coalesced_graph.clear()\n",
    "            for meta_path_pattern in self.meta_path_patterns:\n",
    "                self._cached_coalesced_graph[\n",
    "                    meta_path_pattern\n",
    "                ] = dgl.metapath_reachable_graph(g, meta_path_pattern)\n",
    "\n",
    "        for i, meta_path_pattern in enumerate(self.meta_path_patterns):\n",
    "            new_g = self._cached_coalesced_graph[meta_path_pattern]\n",
    "            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(1))\n",
    "        semantic_embeddings = torch.stack(\n",
    "            semantic_embeddings, dim=1\n",
    "        )  # (N, M, D * K)\n",
    "\n",
    "        return self.semantic_attention(semantic_embeddings)  # (N, D * K)\n",
    "\n",
    "\n",
    "# Relational neighbor aggregation\n",
    "class RelationalAGG(nn.Module):\n",
    "    def __init__(self, g, in_size, out_size, dropout=0.1):\n",
    "        super(RelationalAGG, self).__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        # Transform weights for different types of edges\n",
    "        self.W_T = nn.ModuleDict(\n",
    "            {\n",
    "                name: nn.Linear(in_size, out_size, bias=False)\n",
    "                for name in g.etypes\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Attention weights for different types of edges\n",
    "        self.W_A = nn.ModuleDict(\n",
    "            {name: nn.Linear(out_size, 1, bias=False) for name in g.etypes}\n",
    "        )\n",
    "\n",
    "        # layernorm\n",
    "        self.layernorm = nn.LayerNorm(out_size)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, g, feat_dict):\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in g.canonical_etypes:\n",
    "            g.nodes[dsttype].data[\"h\"] = feat_dict[\n",
    "                dsttype\n",
    "            ]  # nodes' original feature\n",
    "            g.nodes[srctype].data[\"h\"] = feat_dict[srctype]\n",
    "            g.nodes[srctype].data[\"t_h\"] = self.W_T[etype](\n",
    "                feat_dict[srctype]\n",
    "            )  # src nodes' transformed feature\n",
    "\n",
    "            # compute the attention numerator (exp)\n",
    "            g.apply_edges(fn.u_mul_v(\"t_h\", \"h\", \"x\"), etype=etype)\n",
    "            g.edges[etype].data[\"x\"] = torch.exp(\n",
    "                self.W_A[etype](g.edges[etype].data[\"x\"])\n",
    "            )\n",
    "\n",
    "            # first update to compute the attention denominator (\\sum exp)\n",
    "            funcs[etype] = (fn.copy_e(\"x\", \"m\"), fn.sum(\"m\", \"att\"))\n",
    "        g.multi_update_all(funcs, \"sum\")\n",
    "\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in g.canonical_etypes:\n",
    "            g.apply_edges(\n",
    "                fn.e_div_v(\"x\", \"att\", \"att\"), etype=etype\n",
    "            )  # compute attention weights (numerator/denominator)\n",
    "            funcs[etype] = (\n",
    "                fn.u_mul_e(\"h\", \"att\", \"m\"),\n",
    "                fn.sum(\"m\", \"h\"),\n",
    "            )  # \\sum(h0*att) -> h1\n",
    "        # second update to obtain h1\n",
    "        g.multi_update_all(funcs, \"sum\")\n",
    "\n",
    "        # apply activation, layernorm, and dropout\n",
    "        feat_dict = {}\n",
    "        for ntype in g.ntypes:\n",
    "            feat_dict[ntype] = self.dropout(\n",
    "                self.layernorm(F.relu_(g.nodes[ntype].data[\"h\"]))\n",
    "            )  # apply activation, layernorm, and dropout\n",
    "\n",
    "        return feat_dict\n",
    "\n",
    "\n",
    "class TAHIN(nn.Module):\n",
    "    def __init__(\n",
    "        self, g, meta_path_patterns, in_size, out_size, num_heads, dropout\n",
    "    ):\n",
    "        super(TAHIN, self).__init__()\n",
    "\n",
    "        # embeddings for different types of nodes, h0\n",
    "        self.initializer = nn.init.xavier_uniform_\n",
    "        self.feature_dict = nn.ParameterDict(\n",
    "            {\n",
    "                ntype: nn.Parameter(\n",
    "                    self.initializer(torch.empty(g.num_nodes(ntype), in_size))\n",
    "                )\n",
    "                for ntype in g.ntypes\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # relational neighbor aggregation, this produces h1\n",
    "        self.RelationalAGG = RelationalAGG(g, in_size, out_size)\n",
    "\n",
    "        # metapath-based aggregation modules for user and item, this produces h2\n",
    "        self.meta_path_patterns = meta_path_patterns\n",
    "        # one HANLayer for user, one HANLayer for item\n",
    "        self.hans = nn.ModuleDict(\n",
    "            {\n",
    "                key: HANLayer(value, in_size, out_size, num_heads, dropout)\n",
    "                for key, value in self.meta_path_patterns.items()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # layers to combine h0, h1, and h2\n",
    "        # used to update node embeddings\n",
    "        self.user_layer1 = nn.Linear(\n",
    "            (num_heads + 1) * out_size, out_size, bias=True\n",
    "        )\n",
    "        self.user_layer2 = nn.Linear(2 * out_size, out_size, bias=True)\n",
    "        self.item_layer1 = nn.Linear(\n",
    "            (num_heads + 1) * out_size, out_size, bias=True\n",
    "        )\n",
    "        self.item_layer2 = nn.Linear(2 * out_size, out_size, bias=True)\n",
    "\n",
    "        # layernorm\n",
    "        self.layernorm = nn.LayerNorm(out_size)\n",
    "\n",
    "        # network to score the node pairs\n",
    "        self.pred = nn.Linear(out_size, out_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(out_size, 1)\n",
    "\n",
    "    def forward(self, g, user_key, item_key, user_idx, item_idx):\n",
    "        # relational neighbor aggregation, h1\n",
    "        h1 = self.RelationalAGG(g, self.feature_dict)\n",
    "\n",
    "        # metapath-based aggregation, h2\n",
    "        h2 = {}\n",
    "        for key in self.meta_path_patterns.keys():\n",
    "            h2[key] = self.hans[key](g, self.feature_dict[key])\n",
    "\n",
    "        # update node embeddings\n",
    "        user_emb = torch.cat((h1[user_key], h2[user_key]), 1)\n",
    "        item_emb = torch.cat((h1[item_key], h2[item_key]), 1)\n",
    "        user_emb = self.user_layer1(user_emb)\n",
    "        item_emb = self.item_layer1(item_emb)\n",
    "        user_emb = self.user_layer2(\n",
    "            torch.cat((user_emb, self.feature_dict[user_key]), 1)\n",
    "        )\n",
    "        item_emb = self.item_layer2(\n",
    "            torch.cat((item_emb, self.feature_dict[item_key]), 1)\n",
    "        )\n",
    "\n",
    "        # Relu\n",
    "        user_emb = F.relu_(user_emb)\n",
    "        item_emb = F.relu_(item_emb)\n",
    "\n",
    "        # layer norm\n",
    "        user_emb = self.layernorm(user_emb)\n",
    "        item_emb = self.layernorm(item_emb)\n",
    "\n",
    "        # obtain users/items embeddings and their interactions\n",
    "        user_feat = user_emb[user_idx]\n",
    "        item_feat = item_emb[item_idx]\n",
    "        interaction = user_feat * item_feat\n",
    "\n",
    "        # score the node pairs\n",
    "        pred = self.pred(interaction)\n",
    "        pred = self.dropout(pred)  # dropout\n",
    "        pred = self.fc(pred)\n",
    "        pred = torch.sigmoid(pred)\n",
    "\n",
    "        return pred.squeeze(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_ws",
   "language": "python",
   "name": "gnn_ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
